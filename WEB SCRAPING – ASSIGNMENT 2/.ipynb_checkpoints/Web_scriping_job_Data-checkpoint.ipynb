{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb448a7-9bd2-44ad-8fdf-80217c4748bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By  # Correct import\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b4988-1100-4a23-80eb-b3ebd41abf04",
   "metadata": {},
   "source": [
    "# Q1: In this question you have to scrape data using the filters available on the webpage You have to use the location and salary filter.\n",
    "    You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "    You have to scrape the job-title, job-location, company name, experience required.\n",
    "    The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "    The task will be done as shown in the below steps:\n",
    "    1. first get the web page https://www.naukri.com/\n",
    "    2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "    3. Then click the search button.\n",
    "    4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "    5. Then scrape the data for the first 10 jobs results you get.\n",
    "    6. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d773d14-17d8-4e65-b9e6-4e0fff707c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Job Title                                           Location  \\\n",
      "0      Data Scientist  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...   \n",
      "1      Data Scientist  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...   \n",
      "2      Data Scientist                Hybrid - Noida, Gurugram, Bengaluru   \n",
      "3  Sr. Data Scientist  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...   \n",
      "4      Data Scientist                                           Gurugram   \n",
      "5                             Mumbai, Pune, Chennai, Gurugram, Bengaluru   \n",
      "6                                      Delhi / NCR, Hyderabad, Bengaluru   \n",
      "7                                                               Gurugram   \n",
      "8      Data Scientist                                          Faridabad   \n",
      "9      Data Scientist                                              Noida   \n",
      "\n",
      "                                             Company Experience  \n",
      "0                              Futurism Technologies    3-7 Yrs  \n",
      "1                                     Nityo Infotech    3-7 Yrs  \n",
      "2  One of the global analytics and digital soluti...    3-5 Yrs  \n",
      "3                                     Nityo Infotech    3-7 Yrs  \n",
      "4                                      Maruti Suzuki    4-7 Yrs  \n",
      "5                                     Neal Analytics    1-6 Yrs  \n",
      "6                                        Foreign MNC   6-11 Yrs  \n",
      "7                                           Jobpoint    4-7 Yrs  \n",
      "8                                  Rarr Technologies   6-12 Yrs  \n",
      "9                                          Precisely    1-3 Yrs  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up Edge WebDriver\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # This enables using the Chromium-based Edge browser\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "driver.get('https://www.naukri.com/')\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Locate the input field and search for 'Data Scientist'\n",
    "designation = driver.find_element(By.CLASS_NAME, 'suggestor-input').send_keys('Data Scientist')\n",
    "\n",
    "# search button click\n",
    "search = driver.find_element(By.CLASS_NAME, \"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "time.sleep(5)  # Wait for the search results to loa\n",
    "\n",
    "# Apply filters Location & Salray\n",
    "driver.find_element(By.XPATH, \"(//i[@class='ni-icon-unchecked'])[10]\").click()  # Location filter\n",
    "time.sleep(5)  # Wait for the filter to apply\n",
    "driver.find_element(By.XPATH, \"(//i[@class='ni-icon-unchecked'])[13]\").click()  # Salary filter\n",
    "time.sleep(5)  # Wait for the filter to apply\n",
    "\n",
    "jobs = []\n",
    "title_tags = driver.find_elements(By.XPATH, '//a[@class=\"title \"]')\n",
    "location_tags = driver.find_elements(By.XPATH, '//span[@class=\"locWdth\"]')\n",
    "company_tags = driver.find_elements(By.XPATH, '//div[@class=\" row2\"]/span/a[1]')\n",
    "experience_tags = driver.find_elements(By.XPATH, '//span[@class=\"expwdth\"]')\n",
    "\n",
    "# Collect data for the first 10 jobs\n",
    "for i in range(min(10, len(title_tags))):\n",
    "    jobs.append({\n",
    "        'Job Title': title_tags[i].text.strip(),\n",
    "        'Location': location_tags[i].text.strip() if i < len(location_tags) else '',\n",
    "        'Company': company_tags[i].text.strip() if i < len(company_tags) else '',\n",
    "        'Experience': experience_tags[i].text.strip() if i < len(experience_tags) else ''\n",
    "    })\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "df = pd.DataFrame(jobs)\n",
    "df.to_csv('naukri_jobs.csv', index=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579f119-3657-4a90-ac68-a498aa266c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bdbbd48-2096-468e-82c3-c7ebdd8c3e41",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b6c40-87f3-466a-8184-66943a2995a3",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the\n",
    "    job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "    This task will be done in following steps:\n",
    "    1. First get the webpage https://www.shine.com/\n",
    "    2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "    3. Then click the searchbutton.\n",
    "    4. Then scrape the data for the first 10 jobs results you get.\n",
    "    5. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd2bd9f-a683-4cac-a399-a187a77d457d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ElementClickInterceptedException",
     "evalue": "Message: element click intercepted: Element <button type=\"submit\" class=\" btn btn-secondary undefined\">...</button> is not clickable at point (1044, 80). Other element would receive the click: <div class=\"modal_overlay__4_XVc overlayChange topAlign\">...</div>\n  (Session info: MicrosoftEdge=129.0.2792.52)\nStacktrace:\n\tGetHandleVerifier [0x00007FF67FAC7615+12997]\n\tMicrosoft::Applications::Events::EventProperty::empty [0x00007FF67FD41B84+1881780]\n\t(No symbol) [0x00007FF67F87F1BC]\n\t(No symbol) [0x00007FF67F8CCAFA]\n\t(No symbol) [0x00007FF67F8CADAB]\n\t(No symbol) [0x00007FF67F8C8B17]\n\t(No symbol) [0x00007FF67F8C7F2C]\n\t(No symbol) [0x00007FF67F8BC810]\n\t(No symbol) [0x00007FF67F8E7F3A]\n\t(No symbol) [0x00007FF67F8BC08D]\n\t(No symbol) [0x00007FF67F8BBF5D]\n\t(No symbol) [0x00007FF67F8E8220]\n\t(No symbol) [0x00007FF67F8BC08D]\n\t(No symbol) [0x00007FF67F90287A]\n\t(No symbol) [0x00007FF67F8E7BA3]\n\t(No symbol) [0x00007FF67F8BB59C]\n\t(No symbol) [0x00007FF67F8BAA7D]\n\t(No symbol) [0x00007FF67F8BB161]\n\tMicrosoft::Applications::Events::EventProperty::empty [0x00007FF67FC6F994+1021124]\n\t(No symbol) [0x00007FF67F9EEEFF]\n\t(No symbol) [0x00007FF67F9E1697]\n\tMicrosoft::Applications::Events::EventProperty::empty [0x00007FF67FC6E71D+1016397]\n\tMicrosoft::Applications::Events::ILogConfiguration::operator* [0x00007FF67FA67261+329425]\n\tMicrosoft::Applications::Events::ILogConfiguration::operator* [0x00007FF67FA63464+313556]\n\tMicrosoft::Applications::Events::ILogConfiguration::operator* [0x00007FF67FA63599+313865]\n\tMicrosoft::Applications::Events::ILogConfiguration::operator* [0x00007FF67FA5979C+273420]\n\tBaseThreadInitThunk [0x00007FF8EC7A257D+29]\n\tRtlUserThreadStart [0x00007FF8EDD8AF28+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mElementClickInterceptedException\u001b[0m          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mID, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_loc\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msend_keys(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBangalore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Step 4: Click the search button\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m//button[@class=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m btn btn-secondary undefined\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclick\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Wait for the page to load\u001b[39;00m\n\u001b[0;32m     32\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:94\u001b[0m, in \u001b[0;36mWebElement.click\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclick\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Clicks the element.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLICK_ELEMENT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:395\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    393\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    394\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mElementClickInterceptedException\u001b[0m: Message: element click intercepted: Element <button type=\"submit\" class=\" btn btn-secondary undefined\">...</button> is not clickable at point (1044, 80). Other element would receive the click: <div class=\"modal_overlay__4_XVc overlayChange topAlign\">...</div>\n  (Session info: MicrosoftEdge=129.0.2792.52)\nStacktrace:\n\tGetHandleVerifier [0x00007FF67FAC7615+12997]\n\tMicrosoft::Applications::Events::EventProperty::empty [0x00007FF67FD41B84+1881780]\n\t(No symbol) [0x00007FF67F87F1BC]\n\t(No symbol) [0x00007FF67F8CCAFA]\n\t(No symbol) [0x00007FF67F8CADAB]\n\t(No symbol) [0x00007FF67F8C8B17]\n\t(No symbol) [0x00007FF67F8C7F2C]\n\t(No symbol) [0x00007FF67F8BC810]\n\t(No symbol) [0x00007FF67F8E7F3A]\n\t(No symbol) [0x00007FF67F8BC08D]\n\t(No symbol) [0x00007FF67F8BBF5D]\n\t(No symbol) [0x00007FF67F8E8220]\n\t(No symbol) [0x00007FF67F8BC08D]\n\t(No symbol) [0x00007FF67F90287A]\n\t(No symbol) [0x00007FF67F8E7BA3]\n\t(No symbol) [0x00007FF67F8BB59C]\n\t(No symbol) [0x00007FF67F8BAA7D]\n\t(No symbol) [0x00007FF67F8BB161]\n\tMicrosoft::Applications::Events::EventProperty::empty [0x00007FF67FC6F994+1021124]\n\t(No symbol) [0x00007FF67F9EEEFF]\n\t(No symbol) [0x00007FF67F9E1697]\n\tMicrosoft::Applications::Events::EventProperty::empty [0x00007FF67FC6E71D+1016397]\n\tMicrosoft::Applications::Events::ILogConfiguration::operator* [0x00007FF67FA67261+329425]\n\tMicrosoft::Applications::Events::ILogConfiguration::operator* [0x00007FF67FA63464+313556]\n\tMicrosoft::Applications::Events::ILogConfiguration::operator* [0x00007FF67FA63599+313865]\n\tMicrosoft::Applications::Events::ILogConfiguration::operator* [0x00007FF67FA5979C+273420]\n\tBaseThreadInitThunk [0x00007FF8EC7A257D+29]\n\tRtlUserThreadStart [0x00007FF8EDD8AF28+40]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Edge WebDriver with options\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # Use Chromium-based Edge\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "# Set an implicit wait time (e.g., 10 seconds)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "\n",
    "# Step 1: Navigate to Shine website\n",
    "driver.get('https://www.shine.com/')\n",
    "time.sleep(5)\n",
    "\n",
    "driver.find_element(By.CLASS_NAME,\"input\").click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 2: Enter \"Data Scientist\" in the job search field\n",
    "driver.find_element(By.XPATH, \"//input[@id='id_q']\").send_keys('Data Scientist')\n",
    "\n",
    "# Step 3: Enter \"Bangalore\" in the location field\n",
    "driver.find_element(By.ID, 'id_loc').send_keys('Bangalore')\n",
    "\n",
    "# Step 4: Click the search button\n",
    "driver.find_element(By.XPATH, '//button[@class=\" btn btn-secondary undefined\"]').click()\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 5: Scrape job data for the first 10 results\n",
    "job_titles = driver.find_elements(By.XPATH, \"//strong[@class='jobCard_pReplaceH2__xWmHg']\")[:10]\n",
    "locations = driver.find_elements(By.XPATH, \"//div[@class='jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2']\")[:10]\n",
    "companies = driver.find_elements(By.XPATH, \"//div[@class='jobCard_jobCard_cName__mYnow']\")[:10]\n",
    "experiences = driver.find_elements(By.XPATH, \"//div[@class=' jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t']\")[:10]\n",
    "\n",
    "# Collecting data\n",
    "jobs = []\n",
    "for i in range(10):\n",
    "    job = {\n",
    "        'Job Title': job_titles[i].text,\n",
    "        'Location': locations[i].text,\n",
    "        'Company': companies[i].text,\n",
    "        'Experience': experiences[i].text\n",
    "    }\n",
    "    jobs.append(job)\n",
    "\n",
    "# Step 6: Create a DataFrame and print the data\n",
    "df = pd.DataFrame(jobs)\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df.to_csv('shine_job.csv', index=False)\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f8f57-af74-4bf9-94e4-10160b5e1bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e164434c-a9a9-4c17-b1a7-2da4b975149e",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f135b-cbd6-4eab-99f0-fbb276904345",
   "metadata": {},
   "source": [
    "# Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "    https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART:\n",
    "    As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "    1. Rating\n",
    "    2. Review summary\n",
    "    3. Full review\n",
    "    4. You have to scrape this data for first 100reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45db4301-3d7a-41a6-9ee4-a8b1485383a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rating       Review Summary  \\\n",
      "0       5  Best in the market!   \n",
      "1       5             Terrific   \n",
      "2       5       Classy product   \n",
      "3       5    Worth every penny   \n",
      "4       5     Perfect product!   \n",
      "..    ...                  ...   \n",
      "95      5            Excellent   \n",
      "96      5     Perfect product!   \n",
      "97      5            Brilliant   \n",
      "98      5            Must buy!   \n",
      "99      5             Terrific   \n",
      "\n",
      "                                          Full Review  \n",
      "0                                         Good Camera  \n",
      "1                                      Very very good  \n",
      "2   Camera is awesome\\nBest battery backup\\nA perf...  \n",
      "3   Feeling awesome after getting the delivery of ...  \n",
      "4                                        Photos super  \n",
      "..                                                ...  \n",
      "95  A perfect phone and a good battery super camer...  \n",
      "96                         Very nice iPhone 11 i lake  \n",
      "97                                    Fantastic phone  \n",
      "98                                          Excellent  \n",
      "99  Really worth of money. i just love it. It is t...  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Edge WebDriver with options\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # Use Chromium-based Edge\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "# Set an implicit wait time (e.g., 10 seconds)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Navigate to the iPhone 11 review page on Flipkart\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Function to scrape reviews from the current page\n",
    "def scrape_reviews():\n",
    "    review_summaries = driver.find_elements(By.XPATH, \"//p[@class='z9E0IG']\")  # Review summary\n",
    "    ratings = driver.find_elements(By.XPATH, \"//div[@class='XQDdHH Ga3i8K']\")  # Ratings\n",
    "    full_reviews = driver.find_elements(By.XPATH, \"//div[@class='ZmyHeo']\")  # Full review\n",
    "    \n",
    "    reviews = []\n",
    "    num_reviews = min(len(ratings), len(review_summaries), len(full_reviews))\n",
    "    for i in range(num_reviews):\n",
    "        reviews.append({\n",
    "            'Rating': ratings[i].text,\n",
    "            'Review Summary': review_summaries[i].text,\n",
    "            'Full Review': full_reviews[i].text\n",
    "        })\n",
    "    return reviews\n",
    "\n",
    "# Accumulate reviews across multiple pages\n",
    "all_reviews = []\n",
    "while len(all_reviews) < 100:  # Continue until we have 50 reviews\n",
    "    # Scrape reviews from the current page\n",
    "    all_reviews.extend(scrape_reviews())\n",
    "    \n",
    "    # Check if we have enough reviews, if not, click \"Next\" and proceed\n",
    "    if len(all_reviews) < 100:\n",
    "        try:\n",
    "            # Locate and click the \"Next\" button\n",
    "            next_button = driver.find_element(By.XPATH, \"(//span[normalize-space()='Next'])[1]\")\n",
    "            next_button.click()\n",
    "            time.sleep(5)  # Wait for the next page to load\n",
    "        except Exception as e:\n",
    "            print(\"No more pages or error in clicking Next:\", e)\n",
    "            break  # If no more pages or next button can't be clicked, exit loop\n",
    "\n",
    "# Truncate reviews to exactly 50 (if we got more than 50)\n",
    "all_reviews = all_reviews[:100]\n",
    "\n",
    "# Create a DataFrame to store the reviews\n",
    "df_reviews = pd.DataFrame(all_reviews)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_reviews)\n",
    "\n",
    "df_reviews.to_csv('df_reviews.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ebdc8-8c0c-4317-9be3-f705e40c5537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11bdc7a-0fcb-4a48-ad7b-9088680a5893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36806140-f78a-4240-b21e-8db9346f59c5",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651dd2f6-1006-4f3e-aa30-a1932294be76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ab37932-d6f6-40b2-9048-0ccaed61dbac",
   "metadata": {},
   "source": [
    "# Q4: Scrape data forfirst 100 sneakers you find whenyouvisitflipkart.com and search for “sneakers” inthe search field.You have to scrape 3 attributes of each sneaker:\n",
    "    1. Brand\n",
    "    2. ProductDescription\n",
    "    3. Price\n",
    "    As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8502b4c-c1b0-4159-969b-02de8923d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Brand                                        Description   Price\n",
      "0     lejano                                   Sneakers For Men    ₹429\n",
      "1     BRUTON                 Lite Casual Shoes Sneakers For Men    ₹475\n",
      "2       PUMA                                UP Sneakers For Men  ₹2,079\n",
      "3   Red Tape  Sneaker Casual Shoes For Men | Soft Cushion In...  ₹1,187\n",
      "4     BRUTON   ! ComboPack of 2 !Casual Shoes! Sneakers For Men    ₹480\n",
      "..       ...                                                ...     ...\n",
      "95      PUMA                      Puma Onehill Sneakers For Men  ₹1,505\n",
      "96    BRUTON  Combo Pack Of 2 Men’s Casual Shoes Sneakers Fo...    ₹489\n",
      "97    BRUTON      Combo Pack Of 2 Casual Shoes Sneakers For Men    ₹499\n",
      "98  Red Tape  Sneaker Casual Shoes For Men | Soft Cushion In...  ₹1,079\n",
      "99  Red Tape  Casual Sneaker Shoes for Men | Classic Rounded...  ₹1,979\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Edge WebDriver with options\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # Use Chromium-based Edge\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "# Set an implicit wait time (e.g., 10 seconds)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Open Flipkart\n",
    "driver.get('https://www.flipkart.com/')\n",
    "driver.find_element(By.NAME, 'q').send_keys('Sneakers')\n",
    "driver.find_element(By.XPATH, \"(//*[name()='svg'])[1]\").click()\n",
    "\n",
    "sneakers = []\n",
    "max_items = 100  # Maximum number of items to scrape\n",
    "\n",
    "# Loop to handle multiple pages\n",
    "while len(sneakers) < max_items:\n",
    "    # Retrieve elements\n",
    "    brands = driver.find_elements(By.XPATH, \"//div[@class='syl9yP']\")\n",
    "    descriptions = driver.find_elements(By.XPATH, \"//a[contains(@class, 'WKTcLC')]\")\n",
    "    prices = driver.find_elements(By.XPATH, \"//div[@class='Nx9bqj']\")\n",
    "\n",
    "    # Find the minimum length of the lists to avoid index errors\n",
    "    num_items = min(len(brands), len(descriptions), len(prices), max_items - len(sneakers))\n",
    "\n",
    "    # Collect data into a list of dictionaries\n",
    "    for i in range(num_items):\n",
    "        sneakers.append({\n",
    "            'Brand': brands[i].text,\n",
    "            'Description': descriptions[i].text,\n",
    "            'Price': prices[i].text\n",
    "        })\n",
    "\n",
    "    # Check if we have reached the max_items limit\n",
    "    if len(sneakers) >= max_items:\n",
    "        break\n",
    "\n",
    "    # Check if there is a \"Next\" button\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[@class='_9QVEpD']\")  # Update this XPath if necessary\n",
    "        time.sleep(3)  # Wait for the next page to load\n",
    "   \n",
    "\n",
    "# Create a DataFrame and display the results\n",
    "df_sneakers = pd.DataFrame(sneakers[:max_items])  # Limit to max_items\n",
    "print(df_sneakers)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df_sneakers.to_csv('sneakers_data.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e429388a-29b8-4b22-a989-dfc41e96ec48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5558ec7e-58df-4c51-80a3-d02c374bb2c7",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dddab6d-36eb-48db-9131-b39dadf10636",
   "metadata": {},
   "source": [
    "# Q5: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU\n",
    "    Type filter to “Intel Core i7” as shown in the below image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8455624-7a51-4c2e-ba0d-5fe7f6ecd7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title Rating   Price\n",
      "0  HP Laptop 15s, 12th Gen Intel Core i7-1255U, 1...         46,490\n",
      "1  Dell [Smartchoice] Inspiron 5430 Thin & Light ...         47,990\n",
      "2  HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...         52,990\n",
      "3  Dell 15 Thin & Light Laptop, Intel Core i5-123...         59,990\n",
      "4  Acer Aspire Lite 12th Gen Intel Core i7-1255U ...         75,490\n",
      "5  Acer ALG 13th Gen Intel Core i7 Gaming Laptop ...         76,990\n",
      "6  (Refurbished) Dell Latitude 7480 14in FHD Lapt...         52,990\n",
      "7  ASUS Vivobook 15, 15.6\" (39.62cm) FHD, Intel C...         49,990\n",
      "8  Acer Aspire 3 Intel Core i7 12th Gen 1255U - (...         71,990\n",
      "9  Lenovo IdeaPad Slim 3 Intel Core i7 12th Gen 1...         27,531\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize Edge WebDriver with options\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # Use Chromium-based Edge\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "# Set an implicit wait time (e.g., 10 seconds)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "driver.get('https://www.amazon.in/')\n",
    "driver.find_element(By.ID, 'twotabsearchtextbox').send_keys('Laptop')\n",
    "driver.find_element(By.ID, 'nav-search-submit-button').click()\n",
    "\n",
    "# Apply Intel Core i7 filter\n",
    "driver.find_element(By.XPATH, \"//span[text()='Intel Core i7']\").click()\n",
    "\n",
    "# Scrape title, ratings, and price\n",
    "titles = driver.find_elements(By.XPATH, \"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "ratings = driver.find_elements(By.XPATH, \"//span[@class='a-icon-alt']\")\n",
    "prices = driver.find_elements(By.XPATH, \"//span[@class='a-price-whole']\")\n",
    "\n",
    "laptops = []\n",
    "for i in range(10):\n",
    "    laptops.append({\n",
    "        'Title': titles[i].text,\n",
    "        'Rating': ratings[i].text,\n",
    "        'Price': prices[i].text\n",
    "    })\n",
    "\n",
    "df_laptops = pd.DataFrame(laptops)\n",
    "print(df_laptops)\n",
    "\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66279944-0358-4ab3-b10f-33e3376f9299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d3d7099-5ee3-4eac-8c43-f9f6aa4b22e0",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b868243-34c5-48d6-8e84-797786d0335b",
   "metadata": {},
   "source": [
    "# Q6: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "    The above task will be done in following steps:\n",
    "    1. First get the webpagehttps://www.azquotes.com/\n",
    "    2. Click on TopQuote\n",
    "    3. Than scrap a)Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb45aec5-e4e2-419a-bc86-dcee1c2a3e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "No more pages available. Scraping complete.\n",
      "                                                 Quote              Author  \\\n",
      "0                                             Authors:      Michael Porter   \n",
      "1    The essence of strategy is choosing what not t...          Golda Meir   \n",
      "2    One cannot and must not try to erase the past ...  Theodore Roosevelt   \n",
      "3    Patriotism means to stand by the country. It d...      Nelson Mandela   \n",
      "4    Death is something inevitable. When a man has ...        Erma Bombeck   \n",
      "..                                                 ...                 ...   \n",
      "995  Leaders must be close enough to relate to othe...    Sydney J. Harris   \n",
      "996  Regret for the things we did can be tempered b...  Hunter S. Thompson   \n",
      "997  America... just a nation of two hundred millio...            Jim Rohn   \n",
      "998  For every disciplined effort there is a multip...            Ram Dass   \n",
      "999  The spiritual journey is individual, highly pe...            Plutarch   \n",
      "\n",
      "                                         Type  \n",
      "0    Essence, Deep Thought, Transcendentalism  \n",
      "1                   Inspiration, Past, Trying  \n",
      "2                         Country, Peace, War  \n",
      "3          Inspirational, Motivational, Death  \n",
      "4                4th Of July, Food, Patriotic  \n",
      "..                                        ...  \n",
      "995         Love, Inspirational, Motivational  \n",
      "996                    Gun, Two, Qualms About  \n",
      "997     Inspirational, Greatness, Best Effort  \n",
      "998                    Spiritual, Truth, Yoga  \n",
      "999      Inspirational, Leadership, Education  \n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Edge WebDriver with options\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # Use Chromium-based Edge\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "# Set an implicit wait time (e.g., 10 seconds)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Navigate to AzQuotes homepage\n",
    "driver.get('https://www.azquotes.com/')\n",
    "\n",
    "# Click the \"Top Quotes\" link\n",
    "top_quotes_link = driver.find_element(By.XPATH, \"(//a[normalize-space()='Top Quotes'])[1]\")\n",
    "top_quotes_link.click()\n",
    "\n",
    "# Prepare an empty list to store all quotes\n",
    "all_quotes = []\n",
    "page_num = 0\n",
    "\n",
    "while True:\n",
    "    page_num += 1\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    \n",
    "    # Locate quote elements\n",
    "    quote_elements = driver.find_elements(By.CLASS_NAME, 'title')\n",
    "    author_elements = driver.find_elements(By.CLASS_NAME, 'author')\n",
    "    type_elements = driver.find_elements(By.CLASS_NAME, 'tags')\n",
    "    \n",
    "    # Determine the number of available quotes on the current page\n",
    "    num_quotes = min(len(quote_elements), len(author_elements), len(type_elements))\n",
    "    \n",
    "    # Extract data from the current page\n",
    "    for i in range(num_quotes):\n",
    "        quote_text = quote_elements[i].text.strip()\n",
    "        author_text = author_elements[i].text.strip()\n",
    "        type_text = type_elements[i].text.strip()\n",
    "        \n",
    "        all_quotes.append({\n",
    "            'Quote': quote_text,\n",
    "            'Author': author_text,\n",
    "            'Type': type_text\n",
    "        })\n",
    "    \n",
    "    # Try to find and click the \"Next →\" button to go to the next page\n",
    "    next_buttons = driver.find_elements(By.XPATH, \"(//a[contains(text(),'Next →')])[1]\")\n",
    "    \n",
    "    # If no \"Next →\" button is found, exit the loop\n",
    "    if len(next_buttons) == 0:\n",
    "        print(\"No more pages available. Scraping complete.\")\n",
    "        break\n",
    "    \n",
    "    # Click the \"Next →\" button to load the next page\n",
    "    next_buttons[0].click()\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df_quotes = pd.DataFrame(all_quotes)\n",
    "\n",
    "# Display the DataFrame or save to a file\n",
    "print(df_quotes)\n",
    "df_quotes.to_csv('all_quotes.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230ca24-105f-4430-a917-1ea9c8987bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88f239c9-d536-49b4-952d-4c6ec3e49b3e",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae5f59-2cda-4a4b-a578-d910f930d474",
   "metadata": {},
   "source": [
    "# Q7: Write a python program to display list of respected former Prime Ministers of India (i.e. Name,Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/general-knowledge/list-ofall-prime-ministers-of-india-1473165149-1\n",
    "    scrap the mentioned data and make the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e45e2c-ecf3-4cf4-99f8-284fa0f90ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb9d5c-bc28-4f22-809e-dcb25ffb48c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "983b9bde-9258-4485-a040-0e497c6b25ad",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd4e17-6bcf-42fd-a0b6-f5fa151ac854",
   "metadata": {},
   "source": [
    "# Q8: Write a python program to display list of 50 Most expensive cars in the world(i.e. Car name and Price) from https://www.motor1.com/\n",
    "    This task will be done in following steps:\n",
    "    1. First get the webpage https://www.motor1.com/\n",
    "    2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "    3. Then click on 50 most expensive carsin the world..\n",
    "    4. Then scrap thementioned data and make the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49106efd-36b7-45df-9fc8-c310cfbcb854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Step 1: Initialize the WebDriver (Ensure you have the correct driver path)\n",
    "\n",
    "# Set up Edge WebDriver\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # This enables using the Chromium-based Edge browser\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Step 2: Open the webpage\n",
    "    driver.get('https://www.motor1.com/')\n",
    "    \n",
    "    # Maximize window to ensure all elements are visible\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    # Step 3: Wait for the search input to be clickable\n",
    "    search_input = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.ID, 'search_input'))\n",
    "    )\n",
    "    \n",
    "    # Type in the search query\n",
    "    search_input.send_keys('50 most expensive cars')\n",
    "\n",
    "    # Click the search button (assuming there's a button with a type='submit')\n",
    "    search_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[@type='submit']\"))\n",
    "    )\n",
    "    search_button.click()\n",
    "    \n",
    "    # Step 4: Wait for the search results to load and display the desired link\n",
    "    desired_link = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"(//*[name()='svg'])[3]\"))\n",
    "    )\n",
    "    desired_link.click()\n",
    "    \n",
    "    # Step 5: Wait for the new page to load\n",
    "    time.sleep(5)  # You can improve this with WebDriverWait if needed\n",
    "\n",
    "    # Step 6: Scrape the page data using BeautifulSoup\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # Step 7: Find all the <li> tags containing car names and prices\n",
    "    car_list = soup.find_all('li')  # The <li> tags seem to contain the car data\n",
    "\n",
    "    car_data = []\n",
    "    \n",
    "    for car in car_list:\n",
    "        # Extract the name and price directly from the text of the <li> tag\n",
    "        car_info = car.text.strip()  # e.g., \"McLaren Senna GTR - $1.7 Million\"\n",
    "        \n",
    "        # Split the text by '-' to separate the name and price\n",
    "        if ' - ' in car_info:\n",
    "            name, price = car_info.split(' - ', 1)  # Split by ' - ' to separate name and price\n",
    "            car_data.append({'Name': name.strip(), 'Price': price.strip()})\n",
    "\n",
    "    # Step 8: Create a Pandas DataFrame from the scraped data\n",
    "    df = pd.DataFrame(car_data)\n",
    "    print(df)\n",
    "\n",
    "finally:\n",
    "    # Step 9: Quit the driver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31a170-5a9c-43f1-ab10-db542ebfb8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "781762b9-76a4-4ae1-a0d2-619cafa7b329",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
