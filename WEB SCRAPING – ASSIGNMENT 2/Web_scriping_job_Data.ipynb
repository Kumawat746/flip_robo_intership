{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bb448a7-9bd2-44ad-8fdf-80217c4748bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By  \n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b4988-1100-4a23-80eb-b3ebd41abf04",
   "metadata": {},
   "source": [
    "# Q1: In this question you have to scrape data using the filters available on the webpage You have to use the location and salary filter.\n",
    "    You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "    You have to scrape the job-title, job-location, company name, experience required.\n",
    "    The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "    The task will be done as shown in the below steps:\n",
    "    1. first get the web page https://www.naukri.com/\n",
    "    2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "    3. Then click the search button.\n",
    "    4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "    5. Then scrape the data for the first 10 jobs results you get.\n",
    "    6. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d773d14-17d8-4e65-b9e6-4e0fff707c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Job Title                                           Location  \\\n",
      "0  Data Scientist  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...   \n",
      "1  Data Scientist  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...   \n",
      "2  Data Scientist                Hybrid - Noida, Gurugram, Bengaluru   \n",
      "3  Data Scientist  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...   \n",
      "4  Data Scientist                                           Gurugram   \n",
      "5                         Mumbai, Pune, Chennai, Gurugram, Bengaluru   \n",
      "6                                  Delhi / NCR, Hyderabad, Bengaluru   \n",
      "7                                                           Gurugram   \n",
      "8  Data Scientist                                          Faridabad   \n",
      "9  Data Scientist                                              Noida   \n",
      "\n",
      "                                             Company Experience  \n",
      "0                              Futurism Technologies    3-7 Yrs  \n",
      "1                                     Nityo Infotech    3-7 Yrs  \n",
      "2  One of the global analytics and digital soluti...    3-5 Yrs  \n",
      "3                                     Nityo Infotech    3-7 Yrs  \n",
      "4                                      Maruti Suzuki    4-7 Yrs  \n",
      "5                                     Neal Analytics    1-6 Yrs  \n",
      "6                                        Foreign MNC   6-11 Yrs  \n",
      "7                                           Jobpoint    4-7 Yrs  \n",
      "8                                  Rarr Technologies   6-12 Yrs  \n",
      "9                                          Precisely    1-3 Yrs  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up Edge WebDriver\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # This enables using the Chromium-based Edge browser\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "driver.get('https://www.naukri.com/')\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Locate the input field and search for 'Data Scientist'\n",
    "designation = driver.find_element(By.CLASS_NAME, 'suggestor-input').send_keys('Data Scientist')\n",
    "\n",
    "# search button click\n",
    "search = driver.find_element(By.CLASS_NAME, \"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "time.sleep(5)  # Wait for the search results to loa\n",
    "\n",
    "# Apply filters Location & Salray\n",
    "driver.find_element(By.XPATH, \"(//i[@class='ni-icon-unchecked'])[10]\").click()  # Location filter\n",
    "time.sleep(5)  # Wait for the filter to apply\n",
    "driver.find_element(By.XPATH, \"(//i[@class='ni-icon-unchecked'])[13]\").click()  # Salary filter\n",
    "time.sleep(5)  # Wait for the filter to apply\n",
    "\n",
    "jobs = []\n",
    "title_tags = driver.find_elements(By.XPATH, '//a[@class=\"title \"]')\n",
    "location_tags = driver.find_elements(By.XPATH, '//span[@class=\"locWdth\"]')\n",
    "company_tags = driver.find_elements(By.XPATH, '//div[@class=\" row2\"]/span/a[1]')\n",
    "experience_tags = driver.find_elements(By.XPATH, '//span[@class=\"expwdth\"]')\n",
    "\n",
    "# Collect data for the first 10 jobs\n",
    "for i in range(min(10, len(title_tags))):\n",
    "    jobs.append({\n",
    "        'Job Title': title_tags[i].text.strip(),\n",
    "        'Location': location_tags[i].text.strip() if i < len(location_tags) else '',\n",
    "        'Company': company_tags[i].text.strip() if i < len(company_tags) else '',\n",
    "        'Experience': experience_tags[i].text.strip() if i < len(experience_tags) else ''\n",
    "    })\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "df = pd.DataFrame(jobs)\n",
    "df.to_csv('naukri_jobs.csv', index=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579f119-3657-4a90-ac68-a498aa266c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bdbbd48-2096-468e-82c3-c7ebdd8c3e41",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b6c40-87f3-466a-8184-66943a2995a3",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the\n",
    "    job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "    This task will be done in following steps:\n",
    "    1. First get the webpage https://www.shine.com/\n",
    "    2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "    3. Then click the searchbutton.\n",
    "    4. Then scrape the data for the first 10 jobs results you get.\n",
    "    5. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd2bd9f-a683-4cac-a399-a187a77d457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Job Title       Location  \\\n",
      "0       Data Scientist In SINGAPORE  Bangalore\\n+1   \n",
      "1                    Data Scientist  Bangalore\\n+6   \n",
      "2               Lead Data Scientist  Bangalore\\n+8   \n",
      "3                    Data Scientist  Bangalore\\n+9   \n",
      "4                    Data Scientist  Bangalore\\n+2   \n",
      "5                    Data Scientist      Bangalore   \n",
      "6            Data Science Analytics  Bangalore\\n+7   \n",
      "7                 sql data analysis  Bangalore\\n+1   \n",
      "8                      Data Analyst  Bangalore\\n+4   \n",
      "9  Data Catalog with Data Goverance  Bangalore\\n+8   \n",
      "\n",
      "                                  Company    Experience  \n",
      "0                   adal immigrations llp    3 to 8 Yrs  \n",
      "1                           techno endura    0 to 2 Yrs  \n",
      "2               spento papers (india) llp  12 to 22 Yrs  \n",
      "3                  future solution centre    2 to 7 Yrs  \n",
      "4  the christopher's consulting and re...    0 to 2 Yrs  \n",
      "5  eliterecruitments hiring for global...    4 to 8 Yrs  \n",
      "6  mackenzie modern it solutions priva...    5 to 8 Yrs  \n",
      "7         ikaya solutions private limited    3 to 6 Yrs  \n",
      "8                        aryan technology    0 to 4 Yrs  \n",
      "9                     ltimindtree limited   6 to 11 Yrs  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # Use Chromium-based Edge\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "# Set an implicit wait time (e.g., 10 seconds)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "\n",
    "# Step 1: Navigate to Shine website\n",
    "driver.get('https://www.shine.com/')\n",
    "time.sleep(5)\n",
    "\n",
    "driver.find_element(By.CLASS_NAME,\"input\").click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 2: Enter \"Data Scientist\" in the job search field\n",
    "driver.find_element(By.XPATH, \"//input[@id='id_q']\").send_keys('Data Scientist')\n",
    "\n",
    "# Step 3: Enter \"Bangalore\" in the location field\n",
    "driver.find_element(By.ID, 'id_loc').send_keys('Bangalore')\n",
    "\n",
    "# Step 4: Click the search button\n",
    "driver.find_element(By.XPATH, '//button[@class=\" btn btn-secondary undefined\"]').click()\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 5: Scrape job data for the first 10 results\n",
    "job_titles = driver.find_elements(By.XPATH, \"//strong[@class='jobCard_pReplaceH2__xWmHg']\")[:10]\n",
    "locations = driver.find_elements(By.XPATH, \"//div[@class='jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2']\")[:10]\n",
    "companies = driver.find_elements(By.XPATH, \"//div[@class='jobCard_jobCard_cName__mYnow']\")[:10]\n",
    "experiences = driver.find_elements(By.XPATH, \"//div[@class=' jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t']\")[:10]\n",
    "\n",
    "# Collecting data\n",
    "jobs = []\n",
    "for i in range(10):\n",
    "    job = {\n",
    "        'Job Title': job_titles[i].text,\n",
    "        'Location': locations[i].text,\n",
    "        'Company': companies[i].text,\n",
    "        'Experience': experiences[i].text\n",
    "    }\n",
    "    jobs.append(job)\n",
    "\n",
    "# Step 6: Create a DataFrame and print the data\n",
    "df = pd.DataFrame(jobs)\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df.to_csv('shine_job.csv', index=False)\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f8f57-af74-4bf9-94e4-10160b5e1bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e164434c-a9a9-4c17-b1a7-2da4b975149e",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f135b-cbd6-4eab-99f0-fbb276904345",
   "metadata": {},
   "source": [
    "# Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "    https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART:\n",
    "    As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "    1. Rating\n",
    "    2. Review summary\n",
    "    3. Full review\n",
    "    4. You have to scrape this data for first 100reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45db4301-3d7a-41a6-9ee4-a8b1485383a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rating       Review Summary  \\\n",
      "0       5  Best in the market!   \n",
      "1       5             Terrific   \n",
      "2       5       Classy product   \n",
      "3       5    Worth every penny   \n",
      "4       5     Perfect product!   \n",
      "..    ...                  ...   \n",
      "95      5            Excellent   \n",
      "96      5     Perfect product!   \n",
      "97      5            Brilliant   \n",
      "98      5            Must buy!   \n",
      "99      5             Terrific   \n",
      "\n",
      "                                          Full Review  \n",
      "0                                         Good Camera  \n",
      "1                                      Very very good  \n",
      "2   Camera is awesome\\nBest battery backup\\nA perf...  \n",
      "3   Feeling awesome after getting the delivery of ...  \n",
      "4                                        Photos super  \n",
      "..                                                ...  \n",
      "95  A perfect phone and a good battery super camer...  \n",
      "96                         Very nice iPhone 11 i lake  \n",
      "97                                    Fantastic phone  \n",
      "98                                          Excellent  \n",
      "99  Really worth of money. i just love it. It is t...  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Edge WebDriver with options\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # Use Chromium-based Edge\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "# Set an implicit wait time (e.g., 10 seconds)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Navigate to the iPhone 11 review page on Flipkart\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Function to scrape reviews from the current page\n",
    "def scrape_reviews():\n",
    "    review_summaries = driver.find_elements(By.XPATH, \"//p[@class='z9E0IG']\") \n",
    "    ratings = driver.find_elements(By.XPATH, \"//div[@class='XQDdHH Ga3i8K']\")  \n",
    "    full_reviews = driver.find_elements(By.XPATH, \"//div[@class='ZmyHeo']\")  \n",
    "    \n",
    "    reviews = []\n",
    "    num_reviews = min(len(ratings), len(review_summaries), len(full_reviews))\n",
    "    for i in range(num_reviews):\n",
    "        reviews.append({\n",
    "            'Rating': ratings[i].text,\n",
    "            'Review Summary': review_summaries[i].text,\n",
    "            'Full Review': full_reviews[i].text\n",
    "        })\n",
    "    return reviews\n",
    "\n",
    "# Accumulate reviews across multiple pages\n",
    "all_reviews = []\n",
    "while len(all_reviews) < 100:  # Continue until we have 50 reviews\n",
    "    # Scrape reviews from the current page\n",
    "    all_reviews.extend(scrape_reviews())\n",
    "    \n",
    "    # Check if we have enough reviews, if not, click \"Next\" and proceed\n",
    "    if len(all_reviews) < 100:\n",
    "        try:\n",
    "            # Locate and click the \"Next\" button\n",
    "            next_button = driver.find_element(By.XPATH, \"(//span[normalize-space()='Next'])[1]\")\n",
    "            next_button.click()\n",
    "            time.sleep(5)  # Wait for the next page to load\n",
    "        except Exception as e:\n",
    "            print(\"No more pages or error in clicking Next:\", e)\n",
    "            break \n",
    "\n",
    "# Truncate reviews to exactly 50 (if we got more than 50)\n",
    "all_reviews = all_reviews[:100]\n",
    "\n",
    "# Create a DataFrame to store the reviews\n",
    "df_reviews = pd.DataFrame(all_reviews)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_reviews)\n",
    "\n",
    "df_reviews.to_csv('df_reviews.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ebdc8-8c0c-4317-9be3-f705e40c5537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11bdc7a-0fcb-4a48-ad7b-9088680a5893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36806140-f78a-4240-b21e-8db9346f59c5",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651dd2f6-1006-4f3e-aa30-a1932294be76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ab37932-d6f6-40b2-9048-0ccaed61dbac",
   "metadata": {},
   "source": [
    "# Q4: Scrape data forfirst 100 sneakers you find whenyouvisitflipkart.com and search for “sneakers” inthe search field.You have to scrape 3 attributes of each sneaker:\n",
    "    1. Brand\n",
    "    2. ProductDescription\n",
    "    3. Price\n",
    "    As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8502b4c-c1b0-4159-969b-02de8923d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Brand                                        Description    Price\n",
      "0      Deals4you                                 Sneakers For Women     ₹409\n",
      "1          Sparx   SM 852 | Stylish, Comfortable | Sneakers For Men     ₹879\n",
      "2          WROGN                                   Sneakers For Men     ₹699\n",
      "3         CAMPUS                         BROWNIE Sneakers For Women   ₹1,374\n",
      "4       Vellinto            Casual Sneakrs For Men Sneakers For Men     ₹599\n",
      "..           ...                                                ...      ...\n",
      "95      Skechers                        GLIDE-STEP Sneakers For Men   ₹3,369\n",
      "96          Bata                         DUNK E 24 Sneakers For Men     ₹604\n",
      "97          Bata                                   Sneakers For Men     ₹439\n",
      "98      Red Tape  Casual Sneaker Shoes for Men | Classic Rounded...   ₹1,979\n",
      "99  UNDER ARMOUR                                   Sneakers For Men  ₹13,799\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Edge WebDriver with options\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # Use Chromium-based Edge\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "# Set an implicit wait time (e.g., 10 seconds)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Open Flipkart\n",
    "driver.get('https://www.flipkart.com/')\n",
    "driver.find_element(By.NAME, 'q').send_keys('Sneakers')\n",
    "driver.find_element(By.XPATH, \"(//*[name()='svg'])[1]\").click()\n",
    "\n",
    "sneakers = []\n",
    "max_items = 100  # Maximum number of items to scrape\n",
    "\n",
    "# Loop to handle multiple pages\n",
    "while len(sneakers) < max_items:\n",
    "    # Retrieve elements\n",
    "    brands = driver.find_elements(By.XPATH, \"//div[@class='syl9yP']\")\n",
    "    descriptions = driver.find_elements(By.XPATH, \"//a[contains(@class, 'WKTcLC')]\")\n",
    "    prices = driver.find_elements(By.XPATH, \"//div[@class='Nx9bqj']\")\n",
    "\n",
    "    # Find the minimum length of the lists to avoid index errors\n",
    "    num_items = min(len(brands), len(descriptions), len(prices), max_items - len(sneakers))\n",
    "\n",
    "    # Collect data into a list of dictionaries\n",
    "    for i in range(num_items):\n",
    "        sneakers.append({\n",
    "            'Brand': brands[i].text,\n",
    "            'Description': descriptions[i].text,\n",
    "            'Price': prices[i].text\n",
    "        })\n",
    "\n",
    "    # Check if we have reached the max_items limit\n",
    "    if len(sneakers) >= max_items:\n",
    "        break\n",
    "\n",
    "    # Check if there is a \"Next\" button\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[@class='_9QVEpD']\")  \n",
    "        time.sleep(3)  # Wait for the next page to load\n",
    "   \n",
    "\n",
    "# Create a DataFrame and display the results\n",
    "df_sneakers = pd.DataFrame(sneakers[:max_items])  # Limit to max_items\n",
    "print(df_sneakers)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df_sneakers.to_csv('sneakers_data.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e429388a-29b8-4b22-a989-dfc41e96ec48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5558ec7e-58df-4c51-80a3-d02c374bb2c7",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dddab6d-36eb-48db-9131-b39dadf10636",
   "metadata": {},
   "source": [
    "# Q5: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU\n",
    "    Type filter to “Intel Core i7” as shown in the below image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8455624-7a51-4c2e-ba0d-5fe7f6ecd7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title Rating   Price\n",
      "0  HP Laptop 15s, 12th Gen Intel Core i7-1255U, 1...         59,990\n",
      "1  Dell [Smartchoice] Inspiron 5430 Thin & Light ...         75,490\n",
      "2  Acer Aspire Lite 12th Gen Intel Core i7-1255U ...         49,990\n",
      "3  HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...         76,990\n",
      "4  Acer ALG 13th Gen Intel Core i7 Gaming Laptop ...         71,990\n",
      "5  (Refurbished) Dell Latitude 7480 14in FHD Lapt...         27,531\n",
      "6  Acer Aspire Lite 12th Gen Intel Core i7-1255U ...         55,990\n",
      "7  Dell Inspiron 3530 Laptop, 13th Generation Int...         69,890\n",
      "8  Acer Travelmate Business Laptop Intel Core i7-...         44,890\n",
      "9  MSI Thin 15, Intel 13th Gen. Core i7-13620H, 4...         69,990\n"
     ]
    }
   ],
   "source": [
    "\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # Use Chromium-based Edge\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "# Set an implicit wait time (e.g., 10 seconds)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "driver.get('https://www.amazon.in/')\n",
    "driver.find_element(By.ID, 'twotabsearchtextbox').send_keys('Laptop')\n",
    "driver.find_element(By.ID, 'nav-search-submit-button').click()\n",
    "\n",
    "# Apply Intel Core i7 filter\n",
    "driver.find_element(By.XPATH, \"//span[text()='Intel Core i7']\").click()\n",
    "\n",
    "# Scrape title, ratings, and price\n",
    "titles = driver.find_elements(By.XPATH, \"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "ratings = driver.find_elements(By.XPATH, \"//span[@class='a-icon-alt']\")\n",
    "prices = driver.find_elements(By.XPATH, \"//span[@class='a-price-whole']\")\n",
    "\n",
    "laptops = []\n",
    "for i in range(10):\n",
    "    laptops.append({\n",
    "        'Title': titles[i].text,\n",
    "        'Rating': ratings[i].text,\n",
    "        'Price': prices[i].text\n",
    "    })\n",
    "\n",
    "df_laptops = pd.DataFrame(laptops)\n",
    "print(df_laptops)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df_laptops.to_csv('laptop_data.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66279944-0358-4ab3-b10f-33e3376f9299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d3d7099-5ee3-4eac-8c43-f9f6aa4b22e0",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b868243-34c5-48d6-8e84-797786d0335b",
   "metadata": {},
   "source": [
    "# Q6: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "    The above task will be done in following steps:\n",
    "    1. First get the webpagehttps://www.azquotes.com/\n",
    "    2. Click on TopQuote\n",
    "    3. Than scrap a)Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb45aec5-e4e2-419a-bc86-dcee1c2a3e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages available. Scraping complete.\n",
      "                                                 Quote              Author  \\\n",
      "0                                             Authors:      Michael Porter   \n",
      "1    The essence of strategy is choosing what not t...          Golda Meir   \n",
      "2    One cannot and must not try to erase the past ...  Theodore Roosevelt   \n",
      "3    Patriotism means to stand by the country. It d...      Nelson Mandela   \n",
      "4    Death is something inevitable. When a man has ...        Erma Bombeck   \n",
      "..                                                 ...                 ...   \n",
      "995  Leaders must be close enough to relate to othe...    Sydney J. Harris   \n",
      "996  Regret for the things we did can be tempered b...  Hunter S. Thompson   \n",
      "997  America... just a nation of two hundred millio...            Jim Rohn   \n",
      "998  For every disciplined effort there is a multip...            Ram Dass   \n",
      "999  The spiritual journey is individual, highly pe...            Plutarch   \n",
      "\n",
      "                                         Type  \n",
      "0    Essence, Deep Thought, Transcendentalism  \n",
      "1                   Inspiration, Past, Trying  \n",
      "2                         Country, Peace, War  \n",
      "3          Inspirational, Motivational, Death  \n",
      "4                4th Of July, Food, Patriotic  \n",
      "..                                        ...  \n",
      "995         Love, Inspirational, Motivational  \n",
      "996                    Gun, Two, Qualms About  \n",
      "997     Inspirational, Greatness, Best Effort  \n",
      "998                    Spiritual, Truth, Yoga  \n",
      "999      Inspirational, Leadership, Education  \n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Edge WebDriver with options\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # Use Chromium-based Edge\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "# Set an implicit wait time (e.g., 10 seconds)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Navigate to AzQuotes homepage\n",
    "driver.get('https://www.azquotes.com/')\n",
    "\n",
    "# Click the \"Top Quotes\" link\n",
    "top_quotes_link = driver.find_element(By.XPATH, \"(//a[normalize-space()='Top Quotes'])[1]\")\n",
    "top_quotes_link.click()\n",
    "\n",
    "# Prepare an empty list to store all quotes\n",
    "all_quotes = []\n",
    "page_num = 0\n",
    "\n",
    "while True:\n",
    "    page_num += 1\n",
    "    \n",
    "    # Locate quote elements\n",
    "    quote_elements = driver.find_elements(By.CLASS_NAME, 'title')\n",
    "    author_elements = driver.find_elements(By.CLASS_NAME, 'author')\n",
    "    type_elements = driver.find_elements(By.CLASS_NAME, 'tags')\n",
    "    \n",
    "    # Determine the number of available quotes on the current page\n",
    "    num_quotes = min(len(quote_elements), len(author_elements), len(type_elements))\n",
    "    \n",
    "    # Extract data from the current page\n",
    "    for i in range(num_quotes):\n",
    "        quote_text = quote_elements[i].text.strip()\n",
    "        author_text = author_elements[i].text.strip()\n",
    "        type_text = type_elements[i].text.strip()\n",
    "        \n",
    "        all_quotes.append({\n",
    "            'Quote': quote_text,\n",
    "            'Author': author_text,\n",
    "            'Type': type_text\n",
    "        })\n",
    "    \n",
    "    # Try to find and click the \"Next →\" button to go to the next page\n",
    "    next_buttons = driver.find_elements(By.XPATH, \"(//a[contains(text(),'Next →')])[1]\")\n",
    "    \n",
    "    # If no \"Next →\" button is found, exit the loop\n",
    "    if len(next_buttons) == 0:\n",
    "        print(\"No more pages available. Scraping complete.\")\n",
    "        break\n",
    "    \n",
    "    # Click the \"Next →\" button to load the next page\n",
    "    next_buttons[0].click()\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df_quotes = pd.DataFrame(all_quotes)\n",
    "\n",
    "# Display the DataFrame or save to a file\n",
    "print(df_quotes)\n",
    "df_quotes.to_csv('all_quotes.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230ca24-105f-4430-a917-1ea9c8987bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88f239c9-d536-49b4-952d-4c6ec3e49b3e",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae5f59-2cda-4a4b-a578-d910f930d474",
   "metadata": {},
   "source": [
    "# Q7: Write a python program to display list of respected former Prime Ministers of India (i.e. Name,Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/general-knowledge/list-ofall-prime-ministers-of-india-1473165149-1\n",
    "    scrap the mentioned data and make the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e45e2c-ecf3-4cf4-99f8-284fa0f90ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb9d5c-bc28-4f22-809e-dcb25ffb48c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "983b9bde-9258-4485-a040-0e497c6b25ad",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd4e17-6bcf-42fd-a0b6-f5fa151ac854",
   "metadata": {},
   "source": [
    "# Q8: Write a python program to display list of 50 Most expensive cars in the world(i.e. Car name and Price) from https://www.motor1.com/\n",
    "    This task will be done in following steps:\n",
    "    1. First get the webpage https://www.motor1.com/\n",
    "    2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "    3. Then click on 50 most expensive carsin the world..\n",
    "    4. Then scrap thementioned data and make the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781762b9-76a4-4ae1-a0d2-619cafa7b329",
   "metadata": {},
   "source": [
    "# *************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5679ff-f6c5-44bf-868d-3e6607a90022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Car Name                        Price\n",
      "0                           McLaren Senna GTR          Price: $1.7 Million\n",
      "1                                 Czinger 21C          Price: $1.7 Million\n",
      "2                               Ferrari Monza          Price: $1.7 Million\n",
      "3                          Gordon Murray T.33          Price: $1.7 Million\n",
      "4                           Koenigsegg Gemera          Price: $1.8 Million\n",
      "5                          Hennessey Venom F5          Price: $1.9 Million\n",
      "6                             Bentley Bacalar          Price: $1.9 Million\n",
      "7               Hispano Suiza Carmen Boulogne          Price: $2.0 Million\n",
      "8                      Bentley Mulliner Batur          Price: $2.0 Million\n",
      "9                                 SSC Tuatara          Price: $2.1 Million\n",
      "10                                Lotus Evija          Price: $2.3 Million\n",
      "11                        Aston Martin Vulcan          Price: $2.3 Million\n",
      "12                                 Delage D12          Price: $2.3 Million\n",
      "13                        Ferrari Daytona SP3          Price: $2.3 Million\n",
      "14                          McLaren Speedtail          Price: $2.4 Million\n",
      "15                               Rimac Nevera          Price: $2.5 Million\n",
      "16                              Pagani Utopia          Price: $2.5 Million\n",
      "17                       Pininfarina Battista          Price: $2.6 Million\n",
      "18                         Gordon Murray T.50          Price: $2.6 Million\n",
      "19                       Lamborghini Countach          Price: $2.7 Million\n",
      "20              Hennessey Venom F5 Revolution          Price: $2.8 Million\n",
      "21                   Mercedes-AMG Project One          Price: $3.0 Million\n",
      "22                               Zenvo Aurora          Price: $3.0 Million\n",
      "23                        Aston Martin Victor          Price: $3.2 Million\n",
      "24                Hennessey Venom F5 Roadster          Price: $3.4 Million\n",
      "25                           Koenigsegg Jesko          Price: $3.6 million\n",
      "26                                 Aspark Owl          Price: $3.7 Million\n",
      "27                      Aston Martin Valkyrie          Price: $3.9 Million\n",
      "28                  W Motors Lykan Hypersport          Price: $4.5 Million\n",
      "29                              McLaren Solus          Price: $4.7 Million\n",
      "30                        Pagani Huayra Evo R          Price: $4.8 Million\n",
      "31                           Lamborghini Sian          Price: $5.0 Million\n",
      "32                           Koenigsegg CC850          Price: $5.8 Million\n",
      "33            Bugatti Chiron Super Sport 300+          Price: $7.4 Million\n",
      "34  Gordon Murray Automotive T.50s Niki Lauda          Price: $8.0 Million\n",
      "35                  Pagani Huayra Roadster BC          Price: $9.0 Million\n",
      "36                         Lamborghini Veneno         Price: $10.8 Million\n",
      "37                             Bugatti Bolide         Price: $12.8 Million\n",
      "38                  Pininfarina B95 Speedster         Price: $13.4 Million\n",
      "39                            Bugatti Mistral  Price: $28.0 Million (est.)\n",
      "40                               Bugatti Divo    Price: $30 Million (est.)\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Edge WebDriver\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.use_chromium = True  # This enables using the Chromium-based Edge browser\n",
    "driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "# Step 2: Open the webpage\n",
    "driver.get('https://www.motor1.com/')\n",
    "\n",
    "# Wait for the search input to be clickable\n",
    "WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//input[@class='m1-search-panel-input m1-search-form-text']\"))).send_keys('50 most expensive cars')\n",
    "\n",
    "# Click the search button\n",
    "WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//button[@class='m1-search-panel-button m1-search-form-button-animate icon-search-svg']\"))).click()\n",
    "\n",
    "# Wait for the search results to load and display the desired link\n",
    "desired_link = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='text']\")))\n",
    "desired_link.click()\n",
    "\n",
    "# Wait for the new page to load fully\n",
    "WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, \"//h3[@class='subheader']\")))\n",
    "\n",
    "# Step 5: Extract car names and prices\n",
    "car_names = driver.find_elements(By.XPATH, \"//h3[@class='subheader']\")\n",
    "prices = driver.find_elements(By.XPATH, '//strong[contains(text(), \"Price:\")]')\n",
    "\n",
    "# Step 7: Store the data in a list\n",
    "expensive_cars = []\n",
    "num_cars = min(len(car_names), len(prices))  # Use the smaller of the two lists\n",
    "\n",
    "for i in range(num_cars):\n",
    "    expensive_cars.append({\n",
    "        'Car Name': car_names[i].text,\n",
    "        'Price': prices[i].text\n",
    "    })\n",
    "\n",
    "# Step 8: Create a Pandas DataFrame from the scraped data\n",
    "df = pd.DataFrame(expensive_cars)\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('expensive_cars.csv', index=False)\n",
    "\n",
    "# Step 9: Quit the driver\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
